---
date:   2022-03-04 16:48:00 +0900
category: lecture-summary
tags: michigan dl4cv
---

*Justin Johnson님이 미시간대학교에서 진행했던 ‘컴퓨터 비전을 위한 딥러닝 (Deep learning for computer vision)’ 강의를 듣고 정리하겠다. (2019년도 가을 학기 기준)*

### 1. 모수적 접근법과 선형 분류기

**모수적 접근법 (Parametric approach)** 은 함수의 형태를 가정하여 모수에 접근하는 방법이다. 이미지 분류에서는 이미지 $x$가 입력으로 주어졌을 때, 가중치가 $W$인 어떤 함수 $f(x, W)$를 계산하여 클래스별 점수를 얻는 방식으로 생각할 수 있다. 이때 함수의 형태를 $f(x, W) = Wx$와 같이 선형 함수로 가정할 수 있는데 이를 **선형 분류기 (Linear classifier)** 라고 부른다.

선형 분류기를 도입할 경우 3차원 배열 형태의 이미지는 1차원 배열로 펼쳐서 사용한다. 예를 들어 $32 \times 32 \times 3$ 이미지가 주어진다면 모든 픽셀을 일자로 펼쳐 크기가 $3072$인 1차원 배열로 변환하여 함수의 입력값으로 주게 된다. 때로는 $f(x, W) = Wx + b$ 와 같이 함수의 형태에 편향값 (bias) 을 추가하기도 한다.

### 2. 선형 분류기의 이해

다양한 관점으로 선형 분류기를 살펴보며 선형 분류기가 갖는 특성들을 알아보자.

1. 대수적 관점 (Algebraic viewpoint)
    
    기본적으로 함수의 결과값의 각 요소는 $W$의 한 열과 이미지 $x$를 내적한 값이다.
    
    함수의 형태가 $f(x, W) = Wx + b$ 와 같을 때 b를 x의 다음 열에 추가하고 $W$의 마지막 요소로 1을 더하여 함수를 $f(x', W') = W'x'$ 로 변형할 수 있다. 또한 $f(cx, W) = W(cx) = cf(x, W)$으로부터 함수값이 이미지에 대해 선형적으로 변화한다는 것을 알 수 있다.
    
2. 시각적 관점 (Visual viewpoint)
    
    $W$의 각 열은 이미지 $x$와 크기가 동일하기 때문에, 이를 이미지의 원래 크기 (e.g. $32 \times 32 \times 3$) 로 변형할 수 있다. 함수의 결과값의 각 요소는 가중치의 각 열과 이미지를 내적한 값이고, 두 벡터가 평행할 때 두 벡터 간의 내적값이 최대가 된다는 점을 고려할 때, $W$의 각 열을 이미지화한 것은 선형 분류기에서 각 클래스마다 최적의 값을 받을 수 있는 “템플릿 (template)” 이미지로 볼 수 있다. 다시 말해 선형 분류기는 각 카테고리마다 템플릿 이미지와 비교하여 점수를 계산하는, 템플릿 매칭 (Template matching) 을 수행하고 있는 것이다.
    
    선형 분류기는 이미지의 모든 픽셀을 동일하게 간주하기 때문에 이미지의 배경도 많이 참고하게 된다. 그래서 특정 클래스에 해당하는 객체가 뜬금없는 배경에 있을 때 (e.g. 숲 속에 있는 차) 선형 분류기의 예측 성능이 많이 떨어진다. 또한 선형 분류기는 특정 템플릿 이미지 1개와 비교하기 때문에 데이터의 다양한 변형들을 인지하는 데에 어려움이 있다. (편집 필요)
    
3. 지리적 관점 (Geometric viewpoint)
    
    선형 분류기에서 결과값은 해당하는 열의 픽셀값과 선형 관계에 있다. 어떤 픽셀 값이 변화한다면 카테고리별 점수도 이에 따라 선형적으로 바뀌게 된다. 이를 2차원 그래프로 표현한다면 직선으로 나타낼 수 있다. 나아가 여러 픽셀과 어떤 카테고리의 점수의 관계를 표현한다면 고차원 상의 초평면 (hyperplane) 으로 이해할 수 있다.
    
    이러한 특성으로 인해, 아래와 같이 클래스를 구분하는 직선 내지 초평면을 그릴 수 없는 경우에는 선형 분류기가 잘 동작하지 않는다고 이해할 수 있다.
    
    ![선형 분류기의 한계](/assets/images/2022-03-04-lecture-3-linear-classifier/resource-1.png)
    

### 3. 손실 함수와 다중 SVM 손실 함수

선형 분류기는 가중치 $W$가 주어졌을 때 이미지 $x$와 내적하여 점수를 얻는 분류 방식이다. 하지만 $W$의 값은 어떻게 얻어야 할까? 일반적으로 $W$의 값이 좋은지를 평가하는 **손실 함수 (loss function)** 을 정의하고 해당 손실 함수의 값을 최소화하는 $W$의 값을 찾는 방식으로 구한다. 이번 장에서는 손실 함수 위주로 다루고, 다음 장부터 손실 함수의 값을 최소화하는 최적화 기법에 대해 다룬다.

손실 함수는 현재 분류기가 얼마나 좋은지를 나타내는 것으로, 손실 함수의 값이 낮으면 분류기가 좋다는 의미이고 값이 높으면 분류기가 좋지 않다는 뜻이다. 손실 함수는 목적 함수 (objective function), 비용 함수 (cost function) 으로도 불리며, 음수 손실 함수는 보상 함수 (reward function), 이익 함수 (profit function) 와 같은 이름으로 불린다.

$x_i$가 이미지이고 $y_i$가 해당 이미지의 클래스이며 데이터셋 $\{(x_i, y_i)\}_{i=1}^N$ 이 주어졌을 때, 데이터 하나에 대한 손실은 $L_i(f(x_i, W), y_i)$ 로 나타낸다. 데이터셋 전체에 대한 손실은 데이터별 손실의 평균으로, $L=\frac{1}{N}\sum_i{L_i(f(x_i, W), y_i)}$ 로 나타낸다.

**다중 SVM 손실 함수 (Multiclass SVM loss function)** 는 손실 함수의 한 종류로, 정답 클래스의 점수가 다른 클래스 점수보다 낮을 경우 손실을 주는 방식이다. $s=f(x_i, W)$ 라고 할 때 데이터별 SVM 손실 함수는 $L_i=\sum_{j\neq y_i}{\max(0, s_j-s_{y_i}+1)}$ 와 같다. 다시 말해 SVM 손실 함수는 정답 클래스 점수가 다른 클래스 점수보다 충분히 커야 손실을 주지 않는다. 이를 그래프로 나타내면 아래와 같은데 그래프가 문의 경첩과 비슷하다 하여 **힌지 손실 (Hinge loss)** 로도 부른다.

![다중 SVM 손실 함수 - 힌지 손실](/assets/images/2022-03-04-lecture-3-linear-classifier/resource-2.png)

### 4. 가중치 규제

SVM 손실 함수는 정답 클래스의 점수가 다른 클래스 점수보다 충분히 크기만 하면 손실이 0이 되기 때문에, 손실을 0으로 만드는 $W$의 값이 하나 이상이 될 수 있다. 이 경우 $W$의 값은 어떻게 선택해야 할까?

손실은 그 목적에 따라 데이터 손실과 가중치 규제의 두 종류로 나눌 수 있다. 데이터 손실 (Data loss) 은 모델이 학습 데이터에 대해 잘 예측하는지를 평가하는 것이고, **가중치 규제 (Regularization)** 는 모델이 학습 데이터에 과적합하지 않도록 방지하기 위해 사용하는 것이다. 데이터 손실과 함께 가중치 규제도 최소화해야 하므로 $W$의 값을 찾는 것은 다목적 최적화 문제 (multi-objective optimization problem) 이며, 일반적으로 두 손실에 가중치 $\lambda$를 두고 더하여 목적 함수로 사용한다. ($L(W) = \frac{1}{N}\sum_{i=1}^N{L_i(f(x_i, W), y_i)} + \lambda R(W)$)

가중치 규제 방식으로는 L1 가중치 규제 ($R(W)=\sum_k\sum_l |W_{k, l}|$), L2 가중치 규제 ($R(W)=\sum_k\sum_l W_{k, l}^2$), 탄성 그물 (Elastic net, $R(W)=\sum_k\sum_l \beta W_{k, l}^2 + |W_{k, l}|$ 이 있다. L1 규제는 가중치가 적은 속성들에 집중되는 것을 선호하여 가중치 행렬에 0이 많이 포함된다. 반면 L2 규제는 가중치가 어느 정도 분산된 형태를 선호한다. 보다 복잡한 방식으로는 드롭아웃 (Drop out), 배치 정규화 (Batch normalization) 등이 존재한다.

![가중치 규제 - 단순한 모델 선호](/assets/images/2022-03-04-lecture-3-linear-classifier/resource-3.png)

가중치 규제를 하는 이유는 크게 세 가지이다. 첫째, 학습 오차가 낮은 모델만을 추구하지 않고 가중치 규제를 통해 선호하는 모델을 표현하는 것이다. 복잡한 모델보다 단순한 모델에 더 낮은 손실을 출력하여 데이터의 노이즈마저 적합하는 현상을 방지한다. 둘째, 간혹 학습 데이터에서는 잘 동작하지만 새로운 데이터에서는 좋지 않은 성능을 보이는 경우가 있는데, 이러한 **과적합 (Overfitting)** 을 피하기 위한 것이다. 셋째, 곡선성을 부여하여 최적화를 용이하게 하기 위한 것이다.

### 5. 크로스 엔트로피 손실 함수

다중 SVM 손실 함수의 결과값은 대소 관계만 비교할 수 있을 뿐 그 이상의 의미는 담고 있지 않다. **크로스 엔트로피 손실 함수 (Cross-entropy loss function)** 는 분류기의 점수를 확률로써 이해할 수 있도록 고안된 것이다.

확률은 0 이상이어야 하고 합이 1이 되어야 한다는 점을 이용하여, 분류기의 점수 $s=f(x_i;W)$ 를 지수화하고 합이 1이 되도록 정규화하여 분류기의 점수를 확률 형태로 바꿀 수 있다. 이러한 과정을 함수로 나타내면 $P(Y=k|X=x_i)=\cfrac{e^{s_k}}{\sum_j e^{s_j}}$ 와 같은데, 맥스 함수 (max function) 의 미분 가능한 근사라는 의미에서 **소프트맥스 함수 (softmax function)** 이라 한다. 정리하자면 크로스 엔트로피 손실 함수는 $L_i=-\log P(Y=y_i|X=x_i) = -\log(\cfrac{e^{s_{y_i}}}{\sum_j e^{s_j}})$ 로 정의된다.

크로스 엔트로피 손실 함수는 분류기의 점수가 원핫 벡터 (one-hot vector) 일 때만 손실이 0이다. 그래서 SVM 손실 함수와 다르게 정답 클래스의 점수가 가장 높아도 점수 값을 바꾸면 손실에도 변화가 생긴다.